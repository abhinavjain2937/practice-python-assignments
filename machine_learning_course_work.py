# -*- coding: utf-8 -*-
"""machine learning course work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JK-HbScBJ7hhUeOkAlX87krNXlovfE-o

1.     Write a function to generate an m+1 dimensional data set, of size n, consisting of m continuous independent variables (X) and one dependent variable (Y) defined as

                                               yi = xiβ + e

where,

e is a Gaussuan distribution with mean 0 and standard deviation (σ), representing the unexplained variation in Y
β is a random vector of dimensionality m + 1, representing the coefficients of the linear relationship between X and Y, and
∀i ∈ [1, n], xi0 = 1
The function should take the following parameters:

σ: The spread of noise in the output variable
n: The size of the data set
m: The number of indepedent variables
Output from the function should be:

X: An n × m numpy array of independent variable values (with a 1 in the first column)
Y : The n × 1 numpy array of output values
β: The random coefficients used to generatre Y from X
"""



import numpy as np

def generate_dataset(sigma, n, m):
  """
  Generates an m+1 dimensional data set of size n.

  Args:
    sigma: The standard deviation of the Gaussian noise.
    n: The size of the data set.
    m: The number of independent variables.

  Returns:
    X: An n x m numpy array of independent variable values (with a 1 in the first column).
    Y: The n x 1 numpy array of output values.
    beta: The random coefficients used to generate Y from X.
  """
  # Generate random coefficients beta (m+1 coefficients for m independent variables + intercept)
  # randn is used to generate normal distribution or  gaussian distribution (its an numpy function)
  beta = np.random.randn(m + 1, 1)

  # Generate independent variables X (n samples, m variables)
  X = np.random.randn(n, m)
  # Insert a column of ones at the beginning of X for the intercept term
  X = np.insert(X, 0, 1, axis=1)

  # Generate noise e from a Gaussian distribution with mean 0 and standard deviation sigma
  e = np.random.normal(0, sigma, (n, 1))

  # Generate dependent variable Y using the linear model: Y = X * beta + e
  Y = X @ beta + e # Using matrix multiplication (@)

  return X, Y, beta

# Example usage of the function:
# Set parameters
sigma = 2.0  # Standard deviation of noise
n = 100      # Number of samples
m = 3        # Number of independent variables

# Generate the dataset
X_data, Y_data, beta_coefficients = generate_dataset(sigma, n, m)

# Display the shapes of the generated arrays and the coefficients
print("Shape of X:", X_data.shape)
print("Shape of Y:", Y_data.shape)
print("Shape of beta:", beta_coefficients.shape)

# Display the first 5 rows of X and Y, and the generated beta coefficients
print("\nFirst 5 rows of X:")
print(X_data[:5])
print("\nFirst 5 rows of Y:")
print(Y_data[:5])
print("\nGenerated beta coefficients:")
print(beta_coefficients)

"""2. Write a function that learns the parameters of a linear regression line given inputs

X: An n × m numpy array of independent variable values
Y : The n × 1 numpy array of output values
k: the number of iteractions (epochs)
τ : the threshold on change in Cost function value from the previous to current iteration
λ: the learning rate for Gradient Descent
The function should implement the Gradient Descent algorithm as discussed in class that initialises β with random values and then updates these values in each iteraction by moving in the the direction defined by the partial derivative of the cost function with respect to each of the coefficients. The function should use only one loop that ends after a number of iterations (k) or a threshold on the change in cost function value (τ ).

The output should be an m + 1 dimensional vector of coefficients and the final cost function value.


"""

import numpy as np

def linear_regression_gd(X, Y, k, tau, learning_rate):
  """
  Learns the parameters of a linear regression line using Gradient Descent.

  Args:
    X: An n x m numpy array of independent variable values (with a 1 in the first column).
    Y: The n x 1 numpy array of output values.
    k: The number of iterations (epochs).
    tau: The threshold on change in Cost function value.
    learning_rate: The learning rate for Gradient Descent.

  Returns:
    beta: The learned m + 1 dimensional vector of coefficients.
    final_cost: The final cost function value.
  """
  # Initialize beta with random values
  m = X.shape[1] - 1 # Number of independent variables (excluding intercept)
  beta = np.random.randn(m + 1, 1)

  n = X.shape[0] # Number of samples
  cost_history = []

  for i in range(k):
    # Calculate the predicted values
    Y_pred = X @ beta

    # Calculate the error
    error = Y_pred - Y

    # Calculate the gradient
    gradient = (2/n) * X.T @ error

    # Update beta
    beta = beta - learning_rate * gradient

    # Calculate the cost function (Mean Squared Error)
    cost = np.mean(error**2)
    cost_history.append(cost)

    # Check for convergence based on the threshold tau
    if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tau:
      print(f"Converged after {i+1} iterations.")
      break

  return beta, cost

# Example usage (using the data generated in the previous step):
# Set parameters for Gradient Descent
k = 1000  # Number of iterations
tau = 1e-6 # Threshold for convergence
learning_rate = 0.01 # Learning rate

# Learn the coefficients using Gradient Descent
learned_beta, final_cost = linear_regression_gd(X_data, Y_data, k, tau, learning_rate)

# Display the learned coefficients and the final cost
print("\nLearned beta coefficients:")
print(learned_beta)
print("\nFinal cost function value:", final_cost)

# Compare with the true beta coefficients
print("\nTrue beta coefficients:")
print(beta_coefficients)

"""3. Create a report investigating how different values of n and σ impact the ability for your linear regression function to learn the coefficients, β, used to generate the output vector Y ."""





"""# Task
Analyze the impact of the dataset size (`n`) and the standard deviation of noise (`sigma`) on the performance of the `linear_regression_gd` function in learning the true coefficients (`beta_coefficients`) by generating datasets using the `generate_dataset` function with varying `n` and `sigma`, training the model, and evaluating how well the learned coefficients match the true coefficients.

## Define ranges for n and sigma

### Subtask:
Specify a range of values to test for `n` and `sigma`.

**Reasoning**:
Specify the range of values for n and sigma as lists according to the instructions.
"""

# Specify a range of values to test for n and sigma
n_values = [50, 100, 200, 500, 1000]
sigma_values = [0.5, 1.0, 2.0, 5.0, 10.0]

print("n values to test:", n_values)
print("sigma values to test:", sigma_values)

"""## Iterate and experiment

### Subtask:
Loop through different combinations of `n` and `sigma`. For each combination:
*   Generate a dataset using the `generate_dataset` function with the current `n` and `sigma`.
*   Train the linear regression model using the `linear_regression_gd` function on the generated data.
*   Calculate a metric to quantify how well the learned coefficients (`learned_beta`) match the true coefficients (`beta_coefficients`). A common metric is the mean squared error between the true and learned coefficients.

**Reasoning**:
Iterate through the defined ranges of n and sigma, generate data, train the model, calculate the coefficient MSE, and store the results.
"""

results = []
m_fixed = 3
k_fixed = 1000
tau_fixed = 1e-6
learning_rate_fixed = 0.01

for n in n_values:
  for sigma in sigma_values:
    # Generate dataset
    X, Y, beta_coefficients = generate_dataset(sigma, n, m_fixed)

    # Train linear regression model
    learned_beta, final_cost = linear_regression_gd(X, Y, k_fixed, tau_fixed, learning_rate_fixed)

    # Calculate MSE between true and learned coefficients
    mse_beta = np.mean((beta_coefficients - learned_beta)**2)

    # Append results
    results.append({
        'n': n,
        'sigma': sigma,
        'mse_beta': mse_beta,
        'final_cost': final_cost
    })

# Optional: Print first few results to verify
print("First 5 results entries:")
for i in range(min(5, len(results))):
    print(results[i])

"""## Analyze and visualize results

### Subtask:
Analyze the stored results to understand the relationship between `n`, `sigma`, and the model's ability to learn the coefficients. Visualize the results using plots (e.g., a heatmap or scatter plots) to show how the metric changes with varying `n` and `sigma`.

**Reasoning**:
Convert the results list to a pandas DataFrame, create a pivot table, and then visualize the pivot table as a heatmap to analyze the relationship between n, sigma, and the MSE of the learned coefficients.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Convert results list to DataFrame
results_df = pd.DataFrame(results)

# Create a pivot table
pivot_table = results_df.pivot(index='n', columns='sigma', values='mse_beta')

# Create a heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, fmt=".4f", cmap="YlGnBu")
plt.title('MSE of Learned Coefficients vs. Dataset Size (n) and Noise Level (sigma)')
plt.xlabel('Sigma (Standard Deviation of Noise)')
plt.ylabel('Dataset Size (n)')
plt.show()

"""## Report findings

### Subtask:
Summarize the findings from the analysis and visualization in a clear and concise report.

**Reasoning**:
Summarize the findings from the heatmap and pivot table to describe the impact of n and sigma on the MSE of the learned coefficients.
"""

report = """
Analysis Report: Impact of Dataset Size (n) and Noise Level (sigma) on Learned Coefficients

The analysis investigated how the dataset size (n) and the standard deviation of noise (sigma) in the generated data impact the ability of the Gradient Descent linear regression model to learn the true coefficients (beta). The Mean Squared Error (MSE) between the true and learned coefficients (mse_beta) was used as the evaluation metric.

Observations from the Heatmap and Pivot Table:

1.  Impact of Dataset Size (n):
    *   For a fixed noise level (sigma), increasing the dataset size (n) generally leads to a decrease in the MSE of the learned coefficients. This indicates that with more data points, the model is better able to approximate the true underlying linear relationship and thus learn the true coefficients more accurately. This trend is particularly evident at lower sigma values.

2.  Impact of Noise Level (sigma):
    *   For a fixed dataset size (n), increasing the noise level (sigma) generally leads to an increase in the MSE of the learned coefficients. Higher noise levels introduce more randomness and variability in the dependent variable (Y), making it harder for the model to discern the true linear relationship between X and Y, resulting in less accurate coefficient estimates.

Combined Impact:

The heatmap clearly illustrates the combined effect of n and sigma. The lowest MSE values are observed in the top-left region of the heatmap, corresponding to large dataset sizes (high n) and low noise levels (low sigma). Conversely, the highest MSE values are found in the bottom-right region, where the dataset size is small (low n) and the noise level is high (high sigma).

Conclusion:

The results strongly suggest that both dataset size and noise level significantly influence the performance of the linear regression model in learning the true coefficients. A larger dataset size and a lower noise level contribute to more accurate coefficient estimation, leading to a lower Mean Squared Error between the true and learned coefficients. This aligns with the general understanding that more data and less noise improve the reliability of statistical model parameter estimation.
"""

print(report)

"""## Summary:

### Data Analysis Key Findings

*   Increasing the dataset size (`n`) generally leads to a decrease in the Mean Squared Error (MSE) between the true and learned coefficients (`mse_beta`) for a fixed noise level (`sigma`).
*   Increasing the noise level (`sigma`) generally leads to an increase in the MSE of the learned coefficients for a fixed dataset size (`n`).
*   The lowest `mse_beta` values were observed with large dataset sizes and low noise levels.
*   The highest `mse_beta` values were observed with small dataset sizes and high noise levels.

### Insights or Next Steps

*   The results confirm that both dataset size and noise level are critical factors influencing the accuracy of learned coefficients in linear regression.
*   Further analysis could explore the impact of other factors, such as the learning rate or the number of iterations in gradient descent, on the model's performance under varying `n` and `sigma`.

"""